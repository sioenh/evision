{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302afbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b78ed7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 O\\nAD1006929 O\\n\\nThe O\\nobjective O\\nof O\\nthe O\\nresearch O\\nwas O\\nto O\\ninvestigate O\\nand O\\ndetermine O\\nthe O\\nmechanism O\\nthat O\\nproduced O\\nelectrically-generated B-theory\\nelectron M-theory\\nspin M-theory\\npolarization E-theory\\nin O\\nnon-magnetic B-structure\\nsemiconductor M-structure\\nheterostructures E-structure\\n. O\\nElectrically-generated B-theory\\nelectron M-theory\\nspin M-theory\\npolarization E-theory\\nwas O\\nshown O\\nto O\\nbe O\\ninversely O\\nproportional O\\nto O\\nthe O\\nmeasured O\\nmomentum-dependent B-theory\\nspin M-theory\\nsplitting E-theory\\nin O\\nstrained O\\nindium B-material\\ngallium M-material\\narsenide E-material\\n, O\\ncontrary O\\nto O\\ntheoretical O\\nexpectation. O\\nThe O\\nmeasurements O\\nwere O\\nconducted O\\nby O\\nsystematically O\\nvarying O\\nthe O\\ndirection O\\nand O\\nmagnitude O\\nof O\\nthe O\\nin-plane O\\ncurrent O\\nand O\\nnet O\\ndrift O\\nmomentum O\\nin O\\na O\\ndevice O\\nwith O\\na O\\ncross-bargeometry. O\\nThe O\\nrole O\\nof O\\nelectrically-generated O\\nelectron O\\nspin O\\npolarization O\\nin O\\nproducing O\\ndynamic B-structure\\nnuclear M-structure\\npolarization E-structure\\nwas O\\ninvestigated, O\\nand O\\nnuclear B-theory\\nspin M-theory\\npolarization E-theory\\nwas O\\nproduced O\\nthat O\\ncould O\\nbe O\\naligned O\\neither O\\nwith O\\nor O\\nagainst O\\nthe O\\napplied O\\nmagnetic O\\nfield, O\\ndepending O\\non O\\nthe O\\ndirection O\\nof O\\nthe O\\ncurrent. O\\nAseries O\\nof O\\nindium O\\ngallium O\\narsenide O\\nepilayer O\\nsamples O\\nwith O\\nvarying O\\nindium O\\ncomposition O\\nand O\\ndoping S-technology\\ndensity O\\nwere O\\nproduced O\\nand O\\nmeasured O\\nin O\\norder O\\nto O\\ndetermine O\\nhow O\\nchanging O\\nthe O\\nsample O\\nparameters, O\\nsuch O\\nas O\\ns O\\npin-orbit B-theory\\nsplitting E-theory\\n, O\\nspin B-theory\\nrelaxation M-theory\\ntime E-theory\\n, O\\nmomentum B-theory\\nscattering M-theory\\ntime E-theory\\n, O\\nand O\\ncarrier B-theory\\ndensity E-theory\\n,affect O\\nthe O\\nelectrical O\\nspin O\\ngeneration O\\nefficiency. O'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'C:\\Users\\us_Ma\\Desktop\\Share\\material_modified.txt', \"r\", encoding='utf-8') as f:\n",
    "    origin_txt = f.read()\n",
    "    # 按文章分割\n",
    "    origin_txt = origin_txt.replace('\\n\\n\\n\\n','\\n\\n\\n')\n",
    "    artical_list = origin_txt.split('\\n\\n\\n')\n",
    "    # 去除不包含摘要的文章\n",
    "    artical_list = [artical for artical in artical_list if \"O\\n\\nNo O\\nabstract O\\navailable. O\" not in artical]\n",
    "    \n",
    "artical_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f408ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(artical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e9c19b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>artical_id</th>\n",
       "      <th>sentence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We</td>\n",
       "      <td>O</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>present</td>\n",
       "      <td>O</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>an</td>\n",
       "      <td>O</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>B-technology</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monte</td>\n",
       "      <td>I-technology</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word           tag artical_id  sentence_id\n",
       "0        We             O  ADA154997            0\n",
       "1   present             O  ADA154997            0\n",
       "2        an             O  ADA154997            0\n",
       "3  Ensemble  B-technology  ADA154997            0\n",
       "4     Monte  I-technology  ADA154997            0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将标记信息存储为DataFrame\n",
    "df = pd.DataFrame()\n",
    "for artical in artical_list[101:]:\n",
    "    word_list = artical.split('\\n')\n",
    "    artical_id = word_list[1].split(' ')[0]\n",
    "    sentence_id = 0\n",
    "    for word_tagged in word_list[2:]:\n",
    "        \n",
    "        # 处理空字符\n",
    "        if word_tagged == \"\":\n",
    "            continue\n",
    "        word = word_tagged.split(' ')[0]\n",
    "        tag = word_tagged.split(' ')[1] \n",
    "        \n",
    "        # Turn BIOES to BIO\n",
    "        if tag[0] == 'E':\n",
    "            tag = 'I' + tag[1:]\n",
    "        if tag[0] == 'S':\n",
    "            tag = 'B' + tag[1:]\n",
    "        if tag[0] == 'M':\n",
    "            tag = 'I' + tag[1:]   \n",
    "        if tag[2:] == 'phrase':\n",
    "            tag = 'O'\n",
    "        if tag[2:] == 'structure' or tag[2:] == 'experiment':\n",
    "            tag = tag[:2] + 'chemical'\n",
    "        if tag[2:] == 'coponent':\n",
    "            tag = tag[:2] + 'component'\n",
    "        \n",
    "        error_list = ['GaAs', 'Ga', 'InP', 'ZnSe', 'CdTe']\n",
    "        if word in error_list and tag[0] != 'O':\n",
    "            tag = tag[:2] + 'material'\n",
    "        \n",
    "        if word == '':\n",
    "            continue\n",
    "        \n",
    "        # 处理包含标点的word\n",
    "        if word == '.':\n",
    "            new = pd.DataFrame({\n",
    "              'word': word,\n",
    "              'tag': tag,\n",
    "              'artical_id': artical_id,\n",
    "              'sentence_id': sentence_id\n",
    "              },index=[0])\n",
    "            sentence_id += 1   \n",
    "        elif word == ',':\n",
    "            new = pd.DataFrame({\n",
    "              'word': word,\n",
    "              'tag': tag,\n",
    "              'artical_id': artical_id,\n",
    "              'sentence_id': sentence_id\n",
    "              },index=[0]) \n",
    "        elif ',' == word[-1] and not word[-2].isupper():\n",
    "            new = pd.DataFrame([{\n",
    "              'word': word[:-1],\n",
    "              'tag': tag,\n",
    "              'artical_id': artical_id,\n",
    "              'sentence_id': sentence_id\n",
    "              },\n",
    "              {\n",
    "              'word': word[-1],\n",
    "              'tag': 'O',\n",
    "              'artical_id': artical_id,\n",
    "              'sentence_id': sentence_id\n",
    "              }], index=[0,1])\n",
    "                \n",
    "        elif '.' == word[-1] and not word[-2].isupper():\n",
    "            #word = re.sub('[,.]', '', word)\n",
    "            new = pd.DataFrame([{\n",
    "              'word': word[:-1],\n",
    "              'tag': tag,\n",
    "              'artical_id': artical_id,\n",
    "              'sentence_id': sentence_id\n",
    "              },\n",
    "              {\n",
    "              'word': word[-1],\n",
    "              'tag': 'O',\n",
    "              'artical_id': artical_id,\n",
    "              'sentence_id': sentence_id\n",
    "              }], index=[0,1])\n",
    "            sentence_id += 1\n",
    "        else:\n",
    "            new = pd.DataFrame({\n",
    "                  'word': word,\n",
    "                  'tag': tag,\n",
    "                  'artical_id': artical_id,\n",
    "                  'sentence_id': sentence_id\n",
    "                  }, index=[0])\n",
    "        df = df.append(new, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01dc19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-Auxiliary': 1,\n",
       " 'I-Auxiliary': 2,\n",
       " 'B-application': 3,\n",
       " 'I-application': 4,\n",
       " 'B-component': 5,\n",
       " 'I-component': 6,\n",
       " 'B-material': 7,\n",
       " 'I-material': 8,\n",
       " 'B-organization': 9,\n",
       " 'I-organization': 10,\n",
       " 'B-technology': 11,\n",
       " 'I-technology': 12,\n",
       " 'B-theory': 13,\n",
       " 'I-theory': 14}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list_sorted = sorted(list(df.tag.unique()),key=lambda s:s[1:])\n",
    "labels_to_ids = {k: v for v, k in enumerate(tag_list_sorted)}\n",
    "ids_to_labels = {v: k for v, k in enumerate(tag_list_sorted)}\n",
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd5c9a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>artical_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We</td>\n",
       "      <td>O</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>0</td>\n",
       "      <td>We present an Ensemble Monte Carlo ( EMC ) stu...</td>\n",
       "      <td>O,O,O,B-technology,I-technology,I-technology,O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>present</td>\n",
       "      <td>O</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>0</td>\n",
       "      <td>We present an Ensemble Monte Carlo ( EMC ) stu...</td>\n",
       "      <td>O,O,O,B-technology,I-technology,I-technology,O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>an</td>\n",
       "      <td>O</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>0</td>\n",
       "      <td>We present an Ensemble Monte Carlo ( EMC ) stu...</td>\n",
       "      <td>O,O,O,B-technology,I-technology,I-technology,O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>B-technology</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>0</td>\n",
       "      <td>We present an Ensemble Monte Carlo ( EMC ) stu...</td>\n",
       "      <td>O,O,O,B-technology,I-technology,I-technology,O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monte</td>\n",
       "      <td>I-technology</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>0</td>\n",
       "      <td>We present an Ensemble Monte Carlo ( EMC ) stu...</td>\n",
       "      <td>O,O,O,B-technology,I-technology,I-technology,O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word           tag artical_id  sentence_id  \\\n",
       "0        We             O  ADA154997            0   \n",
       "1   present             O  ADA154997            0   \n",
       "2        an             O  ADA154997            0   \n",
       "3  Ensemble  B-technology  ADA154997            0   \n",
       "4     Monte  I-technology  ADA154997            0   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  We present an Ensemble Monte Carlo ( EMC ) stu...   \n",
       "1  We present an Ensemble Monte Carlo ( EMC ) stu...   \n",
       "2  We present an Ensemble Monte Carlo ( EMC ) stu...   \n",
       "3  We present an Ensemble Monte Carlo ( EMC ) stu...   \n",
       "4  We present an Ensemble Monte Carlo ( EMC ) stu...   \n",
       "\n",
       "                                         word_labels  \n",
       "0  O,O,O,B-technology,I-technology,I-technology,O...  \n",
       "1  O,O,O,B-technology,I-technology,I-technology,O...  \n",
       "2  O,O,O,B-technology,I-technology,I-technology,O...  \n",
       "3  O,O,O,B-technology,I-technology,I-technology,O...  \n",
       "4  O,O,O,B-technology,I-technology,I-technology,O...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将单词和标记连接成句\n",
    "df['sentence'] = df.groupby(['artical_id','sentence_id'])['word'].transform(lambda x : ' '.join(x))\n",
    "df['word_labels'] = df.groupby(['artical_id','sentence_id'])['tag'].transform(lambda x : ','.join(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e26ffe67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "      <th>artical_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present an Ensemble Monte Carlo ( EMC ) stu...</td>\n",
       "      <td>O,O,O,B-technology,I-technology,I-technology,O...</td>\n",
       "      <td>ADA154997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the plasma oscillations , to the scattering te...</td>\n",
       "      <td>O,B-technology,I-technology,O,O,O,B-technology...</td>\n",
       "      <td>ADA154997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The transient dynamic response of electrons un...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,B-material,O,O,O,O,O,O,O...</td>\n",
       "      <td>ADA154997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Studies comparing the Gunn effect operation wi...</td>\n",
       "      <td>O,O,O,B-theory,I-theory,I-theory,O,O,O,O,B-the...</td>\n",
       "      <td>N7221801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To overcome difficulties met during earlier [w...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,B-component,I-componen...</td>\n",
       "      <td>N7221801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  We present an Ensemble Monte Carlo ( EMC ) stu...   \n",
       "1  the plasma oscillations , to the scattering te...   \n",
       "2  The transient dynamic response of electrons un...   \n",
       "3  Studies comparing the Gunn effect operation wi...   \n",
       "4  To overcome difficulties met during earlier [w...   \n",
       "\n",
       "                                         word_labels artical_id  \n",
       "0  O,O,O,B-technology,I-technology,I-technology,O...  ADA154997  \n",
       "1  O,B-technology,I-technology,O,O,O,B-technology...  ADA154997  \n",
       "2  O,O,O,O,O,O,O,O,O,O,O,B-material,O,O,O,O,O,O,O...  ADA154997  \n",
       "3  O,O,O,B-theory,I-theory,I-theory,O,O,O,O,B-the...   N7221801  \n",
       "4  O,O,O,O,O,O,O,O,O,O,O,O,B-component,I-componen...   N7221801  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[[\"sentence\", \"word_labels\",\"artical_id\"]].drop_duplicates().reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daac906a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 12\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "# import the pretrained tokenizer of Hugging Face\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9f98e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class dataset():\n",
    "  def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\") \n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                             is_split_into_words=True, \n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True,        # cut the sentence if the length is greater than the MAX_LEN\n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        # convert labels to ids based on label_to_ids\n",
    "        labels = [labels_to_ids[label] for label in word_labels] \n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        # set only labels whose first offset position is 0 and the second is not 0, which means it's the first section of a whole word\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                # overwrite label\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        # Convert the data into a torch.Tensor\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        \n",
    "        return item\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5f7880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (3284, 3)\n",
      "TRAIN Dataset: (2627, 3)\n",
      "TEST Dataset: (657, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into TrainDataset and TestDataset\n",
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200) \n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "# Get the dataset that have passed through the tokenizer\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n",
    "total_set = dataset(data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04bde22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1109,  4884,  4071,  1132, 11267, 18311,  1918, 19975, 20370,\n",
       "           113,  4493,  2069,   114,   117, 11267,   118,  4272,  2702, 20370,\n",
       "           113,   142, 16769,  9565,   114,   117,  1134, 14215,  4493,  2069,\n",
       "          1114,  4272,  8364, 20370,   113,   151, 21148,   114,   117,  1105,\n",
       "         10312,  1193, 11168,  8364, 20370,   113,   152, 20002,  2069,   114,\n",
       "           117,  1107,  1134, 11432,  1104,  1103,  4493,  2069,  4365,  2258,\n",
       "         10645,  2607,  1104,   170,  6307,  7776,  8515, 14797,  4344,   119,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'offset_mapping': tensor([[ 0,  0],\n",
       "         [ 0,  3],\n",
       "         [ 0, 10],\n",
       "         [ 0,  8],\n",
       "         [ 0,  3],\n",
       "         [ 0,  8],\n",
       "         [ 0,  4],\n",
       "         [ 4,  6],\n",
       "         [ 6, 12],\n",
       "         [ 0,  9],\n",
       "         [ 0,  1],\n",
       "         [ 0,  2],\n",
       "         [ 2,  3],\n",
       "         [ 0,  1],\n",
       "         [ 0,  1],\n",
       "         [ 0,  8],\n",
       "         [ 8,  9],\n",
       "         [ 9, 16],\n",
       "         [ 0,  6],\n",
       "         [ 0,  9],\n",
       "         [ 0,  1],\n",
       "         [ 0,  1],\n",
       "         [ 1,  3],\n",
       "         [ 3,  5],\n",
       "         [ 0,  1],\n",
       "         [ 0,  1],\n",
       "         [ 0,  5],\n",
       "         [ 0,  8],\n",
       "         [ 0,  2],\n",
       "         [ 2,  3],\n",
       "         [ 0,  4],\n",
       "         [ 0,  7],\n",
       "         [ 0,  8],\n",
       "         [ 0,  9],\n",
       "         [ 0,  1],\n",
       "         [ 0,  1],\n",
       "         [ 1,  3],\n",
       "         [ 0,  1],\n",
       "         [ 0,  1],\n",
       "         [ 0,  3],\n",
       "         [ 0,  7],\n",
       "         [ 7,  9],\n",
       "         [ 0,  8],\n",
       "         [ 0,  8],\n",
       "         [ 0,  9],\n",
       "         [ 0,  1],\n",
       "         [ 0,  1],\n",
       "         [ 1,  3],\n",
       "         [ 3,  4],\n",
       "         [ 0,  1],\n",
       "         [ 0,  1],\n",
       "         [ 0,  2],\n",
       "         [ 0,  5],\n",
       "         [ 0,  9],\n",
       "         [ 0,  2],\n",
       "         [ 0,  3],\n",
       "         [ 0,  2],\n",
       "         [ 2,  3],\n",
       "         [ 0,  6],\n",
       "         [ 0,  3],\n",
       "         [ 0,  7],\n",
       "         [ 0,  7],\n",
       "         [ 0,  2],\n",
       "         [ 0,  1],\n",
       "         [ 0,  5],\n",
       "         [ 5,  8],\n",
       "         [ 8, 12],\n",
       "         [12, 17],\n",
       "         [ 0,  6],\n",
       "         [ 0,  1],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0]]),\n",
       " 'labels': tensor([-100,    0,    0,    0,    0,   11,   12, -100, -100,   12,    0,   11,\n",
       "         -100,    0,    0,   11, -100, -100,   12,   12,    0,   11, -100, -100,\n",
       "            0,    0,    0,    0,   11, -100,   11,   12,   12,   12,    0,   11,\n",
       "         -100,    0,    0,    0,   11, -100,   12,   12,   12,    0,   11, -100,\n",
       "         -100,    0,    0,    0,    0,    0,    0,    0,    0, -100,    0,    0,\n",
       "            0,    0,    0,    0,    0, -100, -100, -100,    0,    0, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100], dtype=torch.int32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[3]\n",
    "# inputs: The model package all the words into a dict with id, each id represents a word\n",
    "# token_type_ids: 0 for the first sentence and special symbol and 1 for the sencond sentence\n",
    "# attention_mask: set 0 if the corresponding position is [PAD]\n",
    "# offset_mapping: the slice of a subword position based on a whole word\n",
    "# labels: the IOB tag ids of a sentence represented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29d77577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       -100\n",
      "The         0\n",
      "techniques  0\n",
      "employed    0\n",
      "are         0\n",
      "electron    11\n",
      "para        12\n",
      "##ma        -100\n",
      "##gnetic    -100\n",
      "resonance   12\n",
      "(           0\n",
      "EP          11\n",
      "##R         -100\n",
      ")           0\n",
      ",           0\n",
      "electron    11\n",
      "-           -100\n",
      "nuclear     -100\n",
      "double      12\n",
      "resonance   12\n",
      "(           0\n",
      "E           11\n",
      "##ND        -100\n",
      "##OR        -100\n",
      ")           0\n",
      ",           0\n",
      "which       0\n",
      "combines    0\n",
      "EP          11\n",
      "##R         -100\n",
      "with        11\n",
      "nuclear     12\n",
      "magnetic    12\n",
      "resonance   12\n",
      "(           0\n",
      "N           11\n",
      "##MR        -100\n",
      ")           0\n",
      ",           0\n",
      "and         0\n",
      "optical     11\n",
      "##ly        -100\n",
      "detected    12\n",
      "magnetic    12\n",
      "resonance   12\n",
      "(           0\n",
      "O           11\n",
      "##DM        -100\n",
      "##R         -100\n",
      ")           0\n",
      ",           0\n",
      "in          0\n",
      "which       0\n",
      "detection   0\n",
      "of          0\n",
      "the         0\n",
      "EP          0\n",
      "##R         -100\n",
      "occurs      0\n",
      "via         0\n",
      "induced     0\n",
      "changes     0\n",
      "of          0\n",
      "a           0\n",
      "photo       0\n",
      "##lum       -100\n",
      "##ines      -100\n",
      "##cence     -100\n",
      "signal      0\n",
      ".           0\n",
      "[SEP]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n"
     ]
    }
   ],
   "source": [
    "i=3\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[i][\"input_ids\"]), training_set[i][\"labels\"]):\n",
    "      print('{0:10}  {1}'.format(token, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe5d44ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The drift mobility of electron holes in single crystal and polycrystalline MnO , CoO and NiO was determined in the temperature range of about 1000 to 1300C by combined electrical conductivity and either thermal emf or thermogravimetric measurements .'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.sentence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17454de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "306c5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,   # specify how the data loader obtains batches of dataset keys\n",
    "                'shuffle': True,                  # whether the sampler is random\n",
    "                'num_workers': 0                  # turn on multi-process data loading with the specified number of loader worker processes.\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "# DataLoader represents a Python iterable over a dataset, and the most important argument of DataLoader constructor is dataset\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "326420e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the pretrained model of Hugging face\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(labels_to_ids))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73ea6673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = training_set[2]\n",
    "# input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "# attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "# labels = inputs[\"labels\"].unsqueeze(0)\n",
    "\n",
    "# input_ids = input_ids.to(device)\n",
    "# attention_mask = attention_mask.to(device)\n",
    "# labels = labels.to(device)\n",
    "\n",
    "# outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "# initial_loss = outputs[0]\n",
    "# initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41fa7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer could operate the parameters in the model （transfer address）\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36d744c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    ne_count, ne_recognization, ne_irrelevant = {}, {}, {}\n",
    "    ne_recognization_total, ne_irrelevant_total, ne_count_total = 0, 0, 0\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "        output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = output.loss\n",
    "        tr_logits = output.logits\n",
    "        tr_loss += loss.item()\n",
    "        \n",
    "        # print(loss.item())\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels that are not padding labels\n",
    "        active_accuracy = labels.view(-1) != -100# shape (batch_size, seq_len)\n",
    "        # active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "    \n",
    "        # extract the active labels according to active_accuracy\n",
    "        labels_masked = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions_masked = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_labels.extend(labels_masked)\n",
    "        tr_preds.extend(predictions_masked)\n",
    "        \n",
    "        # compute the accuracy of the prediction\n",
    "        tmp_tr_accuracy = accuracy_score(labels_masked.cpu().numpy(), predictions_masked.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "        \n",
    "        tem_ne_count, tem_ne_recognization, tem_ne_irrelevant = entity_count(labels_masked.cpu().numpy(), predictions_masked.cpu().numpy())\n",
    "        for key in tem_ne_count.keys():\n",
    "                ne_count[key] = ne_count.get(key, 0) + tem_ne_count[key]\n",
    "                ne_recognization[key] = ne_recognization.get(key, 0) + tem_ne_recognization[key]\n",
    "                ne_irrelevant[key] = ne_irrelevant.get(key, 0) + tem_ne_irrelevant[key]\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        parameters = list(model.parameters())[:]\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()  # wipe the gradient\n",
    "        loss.backward()        # compute the gradient\n",
    "        optimizer.step()       # update the parameter\n",
    "        # print(list(model.parameters()) == parameters)\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    for key in ne_count.keys():\n",
    "        ne_recognization_total += ne_recognization[key]\n",
    "        ne_irrelevant_total += ne_irrelevant[key]\n",
    "        ne_count_total += ne_count[key]\n",
    "       \n",
    "    recall_ratio_total = ne_recognization_total/ne_count_total\n",
    "    precision_ratio_total = ne_recognization_total/(ne_irrelevant_total+ne_recognization_total)\n",
    "    if precision_ratio_total+recall_ratio_total == 0:\n",
    "        f1_score_total = 0\n",
    "    else:\n",
    "        f1_score_total = 2*precision_ratio_total*recall_ratio_total/(precision_ratio_total+recall_ratio_total)\n",
    "    \n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    print(f\"Name Entity Count: {ne_count_total}, Name Entity Recognization: {ne_recognization_total}, Name Entity Irrelevant: {ne_irrelevant_total}\")\n",
    "    print(f\"Recall Ratio: {recall_ratio_total}  Precision Ratio: {precision_ratio_total}\")\n",
    "    print(f\"F1 Score: {f1_score_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a1ed2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    ne_count, ne_recognization, ne_irrelevant = {}, {}, {}\n",
    "    ne_recognization_total, ne_irrelevant_total, ne_count_total = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "            \n",
    "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = output[0]\n",
    "            eval_logits = output[1]\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)          \n",
    "            \n",
    "            # Step 1: compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            \n",
    "            # compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "            \n",
    "            labels_masked = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions_masked = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(labels_masked)\n",
    "            eval_preds.extend(predictions_masked)\n",
    "            \n",
    "            # compute the accuracy of the prediction\n",
    "            tmp_eval_accuracy = accuracy_score(labels_masked.cpu().numpy(), predictions_masked.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            \n",
    "            tem_ne_count, tem_ne_recognization, tem_ne_irrelevant = entity_count(labels_masked.cpu().numpy(), predictions_masked.cpu().numpy())\n",
    "            for key in tem_ne_count.keys():\n",
    "                ne_count[key] = ne_count.get(key, 0) + tem_ne_count[key]\n",
    "                ne_recognization[key] = ne_recognization.get(key, 0) + tem_ne_recognization[key]\n",
    "                ne_irrelevant[key] = ne_irrelevant.get(key, 0) + tem_ne_irrelevant[key]\n",
    "            \n",
    "\n",
    "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
    "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    \n",
    "    \n",
    "    for key in ne_count.keys():\n",
    "        if ne_count[key] == 0:\n",
    "            recall_ratio = 0\n",
    "        else:\n",
    "            recall_ratio = ne_recognization[key]/ne_count[key]\n",
    "        if ne_irrelevant[key]+ne_recognization[key] == 0:\n",
    "            precision_ratio=0\n",
    "        else:\n",
    "            precision_ratio = ne_recognization[key]/(ne_irrelevant[key]+ne_recognization[key])\n",
    "        if precision_ratio+recall_ratio == 0:\n",
    "            f1_score = 0\n",
    "        else:\n",
    "            f1_score = 2*precision_ratio*recall_ratio/(precision_ratio+recall_ratio)\n",
    "        print(key+'********************')\n",
    "        print(f\"Name Entity Count: {ne_count[key]}, Name Entity Recognization: {ne_recognization[key]}, Name Entity Irrelevant: {ne_irrelevant[key]}\")\n",
    "        print(f\"Recall Ratio: {recall_ratio}  Precision Ratio: {precision_ratio}\")\n",
    "        print(f\"F1 Score: {f1_score}\")\n",
    "        ne_recognization_total += ne_recognization[key]\n",
    "        ne_irrelevant_total += ne_irrelevant[key]\n",
    "        ne_count_total += ne_count[key]\n",
    "       \n",
    "    recall_ratio_total = ne_recognization_total/ne_count_total\n",
    "    precision_ratio_total = ne_recognization_total/(ne_irrelevant_total+ne_recognization_total)\n",
    "    if precision_ratio_total+recall_ratio_total == 0:\n",
    "        f1_score_total = 0\n",
    "    else:\n",
    "        f1_score_total = 2*precision_ratio_total*recall_ratio_total/(precision_ratio_total+recall_ratio_total)\n",
    "    \n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"Name Entity Count: {ne_count_total}, Name Entity Recognization: {ne_recognization_total}, Name Entity Irrelevant: {ne_irrelevant_total}\")\n",
    "    print(f\"Recall Ratio: {recall_ratio_total}  Precision Ratio: {precision_ratio_total}\")\n",
    "    print(f\"F1 Score: {f1_score_total}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5135a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_count(labels, prediction):\n",
    "    label_name_entity = get_name_entity(labels)\n",
    "    prediction_name_entity = get_name_entity(prediction)\n",
    "    ne_count, ne_recognization, ne_irrelevant = {}, {}, {} \n",
    "    for key in label_name_entity.keys():\n",
    "        ne_count[key] = len(label_name_entity[key])\n",
    "        ne_recognization[key] = len(prediction_name_entity[key]&label_name_entity[key])\n",
    "        ne_irrelevant[key] = len(prediction_name_entity[key]-label_name_entity[key])\n",
    "    return ne_count, ne_recognization, ne_irrelevant\n",
    "\n",
    "def get_name_entity(labels):\n",
    "    name_entity_dict = {}\n",
    "    for key in ids_to_labels.keys():\n",
    "        if key%2==1:\n",
    "            name_entity_dict[ids_to_labels[key]]=set()\n",
    "    tem_label = None\n",
    "    for num, label in enumerate(labels):\n",
    "        if tem_label is not None:\n",
    "            if num == len(labels)-1 and label == tem_label + 1:\n",
    "                name_entity_dict[ids_to_labels[tem_label]].add((start, num))\n",
    "                break\n",
    "            if label != tem_label + 1:\n",
    "                name_entity_dict[ids_to_labels[tem_label]].add((start, num - 1))\n",
    "                tem_label = None \n",
    "        if label % 2 != 0:\n",
    "            tem_label = label\n",
    "            start = num\n",
    "            if num == len(labels)-1:\n",
    "                name_entity_dict[ids_to_labels[tem_label]].add((start, num))\n",
    "                break\n",
    "    return name_entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28e97600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Training epoch: 1\n",
      "Training loss epoch: 0.8518817779692737\n",
      "Training accuracy epoch: 0.7735510874426759\n",
      "Name Entity Count: 7022, Name Entity Recognization: 933, Name Entity Irrelevant: 3095\n",
      "Recall Ratio: 0.1328681287382512  Precision Ratio: 0.23162859980139028\n",
      "F1 Score: 0.16886877828054297\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 0, Name Entity Irrelevant: 0\n",
      "Recall Ratio: 0.0  Precision Ratio: 0\n",
      "F1 Score: 0\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 0, Name Entity Irrelevant: 0\n",
      "Recall Ratio: 0.0  Precision Ratio: 0\n",
      "F1 Score: 0\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 0, Name Entity Irrelevant: 1\n",
      "Recall Ratio: 0.0  Precision Ratio: 0.0\n",
      "F1 Score: 0\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 71, Name Entity Irrelevant: 147\n",
      "Recall Ratio: 0.3446601941747573  Precision Ratio: 0.3256880733944954\n",
      "F1 Score: 0.3349056603773585\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 0, Name Entity Irrelevant: 0\n",
      "Recall Ratio: 0.0  Precision Ratio: 0\n",
      "F1 Score: 0\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 4, Name Entity Irrelevant: 10\n",
      "Recall Ratio: 0.018957345971563982  Precision Ratio: 0.2857142857142857\n",
      "F1 Score: 0.03555555555555556\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 473, Name Entity Irrelevant: 805\n",
      "Recall Ratio: 0.5350678733031674  Precision Ratio: 0.3701095461658842\n",
      "F1 Score: 0.4375578168362627\n",
      "Validation Loss: 0.5525561977704381\n",
      "Validation Accuracy: 0.827099262551295\n",
      "Name Entity Count: 1672, Name Entity Recognization: 548, Name Entity Irrelevant: 963\n",
      "Recall Ratio: 0.3277511961722488  Precision Ratio: 0.3626737260092654\n",
      "F1 Score: 0.3443292491360352\n",
      "--------------------------------------------------------\n",
      "Training epoch: 2\n",
      "Training loss epoch: 0.4827950074817195\n",
      "Training accuracy epoch: 0.8448209983643346\n",
      "Name Entity Count: 7022, Name Entity Recognization: 2685, Name Entity Irrelevant: 3732\n",
      "Recall Ratio: 0.3823696952435204  Precision Ratio: 0.41841982234689107\n",
      "F1 Score: 0.3995833023290424\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 7, Name Entity Irrelevant: 8\n",
      "Recall Ratio: 0.050359712230215826  Precision Ratio: 0.4666666666666667\n",
      "F1 Score: 0.09090909090909091\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 0, Name Entity Irrelevant: 0\n",
      "Recall Ratio: 0.0  Precision Ratio: 0\n",
      "F1 Score: 0\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 96, Name Entity Irrelevant: 140\n",
      "Recall Ratio: 0.6  Precision Ratio: 0.4067796610169492\n",
      "F1 Score: 0.4848484848484849\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 122, Name Entity Irrelevant: 228\n",
      "Recall Ratio: 0.5922330097087378  Precision Ratio: 0.3485714285714286\n",
      "F1 Score: 0.43884892086330934\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 0, Name Entity Irrelevant: 0\n",
      "Recall Ratio: 0.0  Precision Ratio: 0\n",
      "F1 Score: 0\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 82, Name Entity Irrelevant: 84\n",
      "Recall Ratio: 0.3886255924170616  Precision Ratio: 0.4939759036144578\n",
      "F1 Score: 0.43501326259946954\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 449, Name Entity Irrelevant: 436\n",
      "Recall Ratio: 0.5079185520361991  Precision Ratio: 0.5073446327683616\n",
      "F1 Score: 0.5076314301865461\n",
      "Validation Loss: 0.414018511452279\n",
      "Validation Accuracy: 0.8617405558306785\n",
      "Name Entity Count: 1672, Name Entity Recognization: 756, Name Entity Irrelevant: 896\n",
      "Recall Ratio: 0.45215311004784686  Precision Ratio: 0.4576271186440678\n",
      "F1 Score: 0.4548736462093863\n",
      "--------------------------------------------------------\n",
      "Training epoch: 3\n",
      "Training loss epoch: 0.3809488112276251\n",
      "Training accuracy epoch: 0.875830349209077\n",
      "Name Entity Count: 7022, Name Entity Recognization: 3541, Name Entity Irrelevant: 3103\n",
      "Recall Ratio: 0.504272287097693  Precision Ratio: 0.5329620710415413\n",
      "F1 Score: 0.5182204009951705\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 28, Name Entity Irrelevant: 31\n",
      "Recall Ratio: 0.2014388489208633  Precision Ratio: 0.4745762711864407\n",
      "F1 Score: 0.2828282828282828\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 0, Name Entity Irrelevant: 0\n",
      "Recall Ratio: 0.0  Precision Ratio: 0\n",
      "F1 Score: 0\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 103, Name Entity Irrelevant: 114\n",
      "Recall Ratio: 0.64375  Precision Ratio: 0.47465437788018433\n",
      "F1 Score: 0.5464190981432361\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 144, Name Entity Irrelevant: 171\n",
      "Recall Ratio: 0.6990291262135923  Precision Ratio: 0.45714285714285713\n",
      "F1 Score: 0.5527831094049904\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 1, Name Entity Irrelevant: 0\n",
      "Recall Ratio: 0.02040816326530612  Precision Ratio: 1.0\n",
      "F1 Score: 0.039999999999999994\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 87, Name Entity Irrelevant: 78\n",
      "Recall Ratio: 0.41232227488151657  Precision Ratio: 0.5272727272727272\n",
      "F1 Score: 0.46276595744680843\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 466, Name Entity Irrelevant: 351\n",
      "Recall Ratio: 0.5271493212669683  Precision Ratio: 0.5703794369645043\n",
      "F1 Score: 0.5479129923574367\n",
      "Validation Loss: 0.3856591241304001\n",
      "Validation Accuracy: 0.8742126463021819\n",
      "Name Entity Count: 1672, Name Entity Recognization: 829, Name Entity Irrelevant: 745\n",
      "Recall Ratio: 0.4958133971291866  Precision Ratio: 0.5266836086404066\n",
      "F1 Score: 0.5107825015403573\n",
      "--------------------------------------------------------\n",
      "Training epoch: 4\n",
      "Training loss epoch: 0.3167493431857138\n",
      "Training accuracy epoch: 0.89687768995007\n",
      "Name Entity Count: 7022, Name Entity Recognization: 4074, Name Entity Irrelevant: 2681\n",
      "Recall Ratio: 0.5801765878667047  Precision Ratio: 0.6031088082901555\n",
      "F1 Score: 0.5914204834143862\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 45, Name Entity Irrelevant: 46\n",
      "Recall Ratio: 0.3237410071942446  Precision Ratio: 0.4945054945054945\n",
      "F1 Score: 0.3913043478260869\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 0, Name Entity Irrelevant: 0\n",
      "Recall Ratio: 0.0  Precision Ratio: 0\n",
      "F1 Score: 0\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 100, Name Entity Irrelevant: 84\n",
      "Recall Ratio: 0.625  Precision Ratio: 0.5434782608695652\n",
      "F1 Score: 0.5813953488372093\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 142, Name Entity Irrelevant: 130\n",
      "Recall Ratio: 0.6893203883495146  Precision Ratio: 0.5220588235294118\n",
      "F1 Score: 0.5941422594142259\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 11, Name Entity Irrelevant: 5\n",
      "Recall Ratio: 0.22448979591836735  Precision Ratio: 0.6875\n",
      "F1 Score: 0.3384615384615384\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 101, Name Entity Irrelevant: 79\n",
      "Recall Ratio: 0.4786729857819905  Precision Ratio: 0.5611111111111111\n",
      "F1 Score: 0.5166240409207161\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 507, Name Entity Irrelevant: 441\n",
      "Recall Ratio: 0.5735294117647058  Precision Ratio: 0.5348101265822784\n",
      "F1 Score: 0.5534934497816594\n",
      "Validation Loss: 0.3594601230821813\n",
      "Validation Accuracy: 0.8831874922606547\n",
      "Name Entity Count: 1672, Name Entity Recognization: 906, Name Entity Irrelevant: 785\n",
      "Recall Ratio: 0.5418660287081339  Precision Ratio: 0.5357776463630988\n",
      "F1 Score: 0.5388046387154325\n",
      "--------------------------------------------------------\n",
      "Training epoch: 5\n",
      "Training loss epoch: 0.26950234454689603\n",
      "Training accuracy epoch: 0.9108514275303662\n",
      "Name Entity Count: 7022, Name Entity Recognization: 4330, Name Entity Irrelevant: 2515\n",
      "Recall Ratio: 0.616633437767018  Precision Ratio: 0.6325785244704164\n",
      "F1 Score: 0.6245042186485902\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 50, Name Entity Irrelevant: 52\n",
      "Recall Ratio: 0.3597122302158273  Precision Ratio: 0.49019607843137253\n",
      "F1 Score: 0.4149377593360995\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 0, Name Entity Irrelevant: 0\n",
      "Recall Ratio: 0.0  Precision Ratio: 0\n",
      "F1 Score: 0\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 116, Name Entity Irrelevant: 126\n",
      "Recall Ratio: 0.725  Precision Ratio: 0.4793388429752066\n",
      "F1 Score: 0.5771144278606964\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 143, Name Entity Irrelevant: 128\n",
      "Recall Ratio: 0.6941747572815534  Precision Ratio: 0.5276752767527675\n",
      "F1 Score: 0.5995807127882599\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 21, Name Entity Irrelevant: 11\n",
      "Recall Ratio: 0.42857142857142855  Precision Ratio: 0.65625\n",
      "F1 Score: 0.5185185185185185\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 114, Name Entity Irrelevant: 100\n",
      "Recall Ratio: 0.5402843601895735  Precision Ratio: 0.5327102803738317\n",
      "F1 Score: 0.5364705882352941\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 509, Name Entity Irrelevant: 380\n",
      "Recall Ratio: 0.5757918552036199  Precision Ratio: 0.5725534308211474\n",
      "F1 Score: 0.5741680767061478\n",
      "Validation Loss: 0.376564917878579\n",
      "Validation Accuracy: 0.8819260861082985\n",
      "Name Entity Count: 1672, Name Entity Recognization: 953, Name Entity Irrelevant: 797\n",
      "Recall Ratio: 0.5699760765550239  Precision Ratio: 0.5445714285714286\n",
      "F1 Score: 0.5569842197545295\n",
      "--------------------------------------------------------\n",
      "Training epoch: 6\n",
      "Training loss epoch: 0.22852173366329886\n",
      "Training accuracy epoch: 0.9261072937489773\n",
      "Name Entity Count: 7022, Name Entity Recognization: 4678, Name Entity Irrelevant: 2185\n",
      "Recall Ratio: 0.6661919681002564  Precision Ratio: 0.6816261110301617\n",
      "F1 Score: 0.6738206697875405\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 64, Name Entity Irrelevant: 65\n",
      "Recall Ratio: 0.460431654676259  Precision Ratio: 0.49612403100775193\n",
      "F1 Score: 0.47761194029850745\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 0, Name Entity Irrelevant: 0\n",
      "Recall Ratio: 0.0  Precision Ratio: 0\n",
      "F1 Score: 0\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 103, Name Entity Irrelevant: 91\n",
      "Recall Ratio: 0.64375  Precision Ratio: 0.5309278350515464\n",
      "F1 Score: 0.5819209039548022\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 130, Name Entity Irrelevant: 106\n",
      "Recall Ratio: 0.6310679611650486  Precision Ratio: 0.5508474576271186\n",
      "F1 Score: 0.5882352941176471\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 25, Name Entity Irrelevant: 17\n",
      "Recall Ratio: 0.5102040816326531  Precision Ratio: 0.5952380952380952\n",
      "F1 Score: 0.5494505494505494\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 108, Name Entity Irrelevant: 75\n",
      "Recall Ratio: 0.5118483412322274  Precision Ratio: 0.5901639344262295\n",
      "F1 Score: 0.5482233502538071\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 519, Name Entity Irrelevant: 442\n",
      "Recall Ratio: 0.5871040723981901  Precision Ratio: 0.5400624349635796\n",
      "F1 Score: 0.5626016260162602\n",
      "Validation Loss: 0.39603525994499117\n",
      "Validation Accuracy: 0.8836017235009742\n",
      "Name Entity Count: 1672, Name Entity Recognization: 949, Name Entity Irrelevant: 796\n",
      "Recall Ratio: 0.5675837320574163  Precision Ratio: 0.543839541547278\n",
      "F1 Score: 0.5554580040971613\n",
      "--------------------------------------------------------\n",
      "Training epoch: 7\n",
      "Training loss epoch: 0.18713977761340864\n",
      "Training accuracy epoch: 0.9397214377905287\n",
      "Name Entity Count: 7022, Name Entity Recognization: 5047, Name Entity Irrelevant: 1910\n",
      "Recall Ratio: 0.7187410994018798  Precision Ratio: 0.7254563748742274\n",
      "F1 Score: 0.7220831246870305\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 72, Name Entity Irrelevant: 82\n",
      "Recall Ratio: 0.5179856115107914  Precision Ratio: 0.4675324675324675\n",
      "F1 Score: 0.4914675767918088\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 1, Name Entity Irrelevant: 1\n",
      "Recall Ratio: 0.043478260869565216  Precision Ratio: 0.5\n",
      "F1 Score: 0.08\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 94, Name Entity Irrelevant: 67\n",
      "Recall Ratio: 0.5875  Precision Ratio: 0.5838509316770186\n",
      "F1 Score: 0.5856697819314642\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 136, Name Entity Irrelevant: 104\n",
      "Recall Ratio: 0.6601941747572816  Precision Ratio: 0.5666666666666667\n",
      "F1 Score: 0.6098654708520179\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 29, Name Entity Irrelevant: 16\n",
      "Recall Ratio: 0.5918367346938775  Precision Ratio: 0.6444444444444445\n",
      "F1 Score: 0.6170212765957447\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 100, Name Entity Irrelevant: 58\n",
      "Recall Ratio: 0.47393364928909953  Precision Ratio: 0.6329113924050633\n",
      "F1 Score: 0.5420054200542005\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 492, Name Entity Irrelevant: 384\n",
      "Recall Ratio: 0.5565610859728507  Precision Ratio: 0.5616438356164384\n",
      "F1 Score: 0.5590909090909091\n",
      "Validation Loss: 0.38172710497455253\n",
      "Validation Accuracy: 0.8869192882347333\n",
      "Name Entity Count: 1672, Name Entity Recognization: 924, Name Entity Irrelevant: 712\n",
      "Recall Ratio: 0.5526315789473685  Precision Ratio: 0.5647921760391198\n",
      "F1 Score: 0.5586457073760581\n",
      "--------------------------------------------------------\n",
      "Training epoch: 8\n",
      "Training loss epoch: 0.1638857625650637\n",
      "Training accuracy epoch: 0.9471706271608714\n",
      "Name Entity Count: 7022, Name Entity Recognization: 5288, Name Entity Irrelevant: 1646\n",
      "Recall Ratio: 0.7530618057533466  Precision Ratio: 0.7626189789443323\n",
      "F1 Score: 0.7578102608197191\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 70, Name Entity Irrelevant: 82\n",
      "Recall Ratio: 0.5035971223021583  Precision Ratio: 0.4605263157894737\n",
      "F1 Score: 0.4810996563573883\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 1, Name Entity Irrelevant: 3\n",
      "Recall Ratio: 0.043478260869565216  Precision Ratio: 0.25\n",
      "F1 Score: 0.07407407407407408\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 107, Name Entity Irrelevant: 101\n",
      "Recall Ratio: 0.66875  Precision Ratio: 0.5144230769230769\n",
      "F1 Score: 0.5815217391304347\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 141, Name Entity Irrelevant: 116\n",
      "Recall Ratio: 0.6844660194174758  Precision Ratio: 0.5486381322957199\n",
      "F1 Score: 0.6090712742980562\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 28, Name Entity Irrelevant: 19\n",
      "Recall Ratio: 0.5714285714285714  Precision Ratio: 0.5957446808510638\n",
      "F1 Score: 0.5833333333333334\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 122, Name Entity Irrelevant: 85\n",
      "Recall Ratio: 0.5781990521327014  Precision Ratio: 0.5893719806763285\n",
      "F1 Score: 0.5837320574162679\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 529, Name Entity Irrelevant: 371\n",
      "Recall Ratio: 0.5984162895927602  Precision Ratio: 0.5877777777777777\n",
      "F1 Score: 0.59304932735426\n",
      "Validation Loss: 0.4255706690616996\n",
      "Validation Accuracy: 0.8845542035387827\n",
      "Name Entity Count: 1672, Name Entity Recognization: 998, Name Entity Irrelevant: 777\n",
      "Recall Ratio: 0.59688995215311  Precision Ratio: 0.5622535211267605\n",
      "F1 Score: 0.5790542500725268\n",
      "--------------------------------------------------------\n",
      "Training epoch: 9\n",
      "Training loss epoch: 0.13183262314308775\n",
      "Training accuracy epoch: 0.959055495217591\n",
      "Name Entity Count: 7022, Name Entity Recognization: 5549, Name Entity Irrelevant: 1408\n",
      "Recall Ratio: 0.7902307035032754  Precision Ratio: 0.7976139140434095\n",
      "F1 Score: 0.79390514342943\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 64, Name Entity Irrelevant: 69\n",
      "Recall Ratio: 0.460431654676259  Precision Ratio: 0.48120300751879697\n",
      "F1 Score: 0.47058823529411764\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 1, Name Entity Irrelevant: 2\n",
      "Recall Ratio: 0.043478260869565216  Precision Ratio: 0.3333333333333333\n",
      "F1 Score: 0.07692307692307691\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 101, Name Entity Irrelevant: 74\n",
      "Recall Ratio: 0.63125  Precision Ratio: 0.5771428571428572\n",
      "F1 Score: 0.6029850746268657\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 129, Name Entity Irrelevant: 75\n",
      "Recall Ratio: 0.6262135922330098  Precision Ratio: 0.6323529411764706\n",
      "F1 Score: 0.6292682926829269\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 28, Name Entity Irrelevant: 13\n",
      "Recall Ratio: 0.5714285714285714  Precision Ratio: 0.6829268292682927\n",
      "F1 Score: 0.6222222222222223\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 116, Name Entity Irrelevant: 75\n",
      "Recall Ratio: 0.5497630331753555  Precision Ratio: 0.6073298429319371\n",
      "F1 Score: 0.5771144278606966\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 522, Name Entity Irrelevant: 374\n",
      "Recall Ratio: 0.5904977375565611  Precision Ratio: 0.5825892857142857\n",
      "F1 Score: 0.5865168539325842\n",
      "Validation Loss: 0.4358600366071198\n",
      "Validation Accuracy: 0.8873378503087544\n",
      "Name Entity Count: 1672, Name Entity Recognization: 961, Name Entity Irrelevant: 682\n",
      "Recall Ratio: 0.5747607655502392  Precision Ratio: 0.5849056603773585\n",
      "F1 Score: 0.579788838612368\n",
      "--------------------------------------------------------\n",
      "Training epoch: 10\n",
      "Training loss epoch: 0.11322268310821418\n",
      "Training accuracy epoch: 0.9654328664217193\n",
      "Name Entity Count: 7022, Name Entity Recognization: 5769, Name Entity Irrelevant: 1178\n",
      "Recall Ratio: 0.8215608088863572  Precision Ratio: 0.8304304016122067\n",
      "F1 Score: 0.8259717946882382\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 76, Name Entity Irrelevant: 86\n",
      "Recall Ratio: 0.5467625899280576  Precision Ratio: 0.4691358024691358\n",
      "F1 Score: 0.5049833887043189\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 1, Name Entity Irrelevant: 10\n",
      "Recall Ratio: 0.043478260869565216  Precision Ratio: 0.09090909090909091\n",
      "F1 Score: 0.0588235294117647\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 101, Name Entity Irrelevant: 85\n",
      "Recall Ratio: 0.63125  Precision Ratio: 0.543010752688172\n",
      "F1 Score: 0.583815028901734\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 135, Name Entity Irrelevant: 82\n",
      "Recall Ratio: 0.6553398058252428  Precision Ratio: 0.6221198156682027\n",
      "F1 Score: 0.6382978723404256\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 29, Name Entity Irrelevant: 17\n",
      "Recall Ratio: 0.5918367346938775  Precision Ratio: 0.6304347826086957\n",
      "F1 Score: 0.6105263157894736\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 113, Name Entity Irrelevant: 78\n",
      "Recall Ratio: 0.5355450236966824  Precision Ratio: 0.5916230366492147\n",
      "F1 Score: 0.5621890547263682\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 511, Name Entity Irrelevant: 390\n",
      "Recall Ratio: 0.5780542986425339  Precision Ratio: 0.5671476137624861\n",
      "F1 Score: 0.572549019607843\n",
      "Validation Loss: 0.44700600625363274\n",
      "Validation Accuracy: 0.8864511793017212\n",
      "Name Entity Count: 1672, Name Entity Recognization: 966, Name Entity Irrelevant: 748\n",
      "Recall Ratio: 0.5777511961722488  Precision Ratio: 0.5635939323220537\n",
      "F1 Score: 0.570584760779681\n",
      "--------------------------------------------------------\n",
      "Training epoch: 11\n",
      "Training loss epoch: 0.0966008875293262\n",
      "Training accuracy epoch: 0.9696098836381284\n",
      "Name Entity Count: 7022, Name Entity Recognization: 5890, Name Entity Irrelevant: 1086\n",
      "Recall Ratio: 0.8387923668470522  Precision Ratio: 0.8443233944954128\n",
      "F1 Score: 0.8415487926846692\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 75, Name Entity Irrelevant: 71\n",
      "Recall Ratio: 0.539568345323741  Precision Ratio: 0.5136986301369864\n",
      "F1 Score: 0.5263157894736842\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 3, Name Entity Irrelevant: 14\n",
      "Recall Ratio: 0.13043478260869565  Precision Ratio: 0.17647058823529413\n",
      "F1 Score: 0.15\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 99, Name Entity Irrelevant: 84\n",
      "Recall Ratio: 0.61875  Precision Ratio: 0.5409836065573771\n",
      "F1 Score: 0.577259475218659\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 139, Name Entity Irrelevant: 77\n",
      "Recall Ratio: 0.6747572815533981  Precision Ratio: 0.6435185185185185\n",
      "F1 Score: 0.6587677725118484\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 28, Name Entity Irrelevant: 18\n",
      "Recall Ratio: 0.5714285714285714  Precision Ratio: 0.6086956521739131\n",
      "F1 Score: 0.5894736842105263\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 109, Name Entity Irrelevant: 69\n",
      "Recall Ratio: 0.5165876777251185  Precision Ratio: 0.6123595505617978\n",
      "F1 Score: 0.5604113110539846\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 537, Name Entity Irrelevant: 444\n",
      "Recall Ratio: 0.6074660633484162  Precision Ratio: 0.5474006116207951\n",
      "F1 Score: 0.5758713136729222\n",
      "Validation Loss: 0.4590605170644521\n",
      "Validation Accuracy: 0.8859148309832775\n",
      "Name Entity Count: 1672, Name Entity Recognization: 990, Name Entity Irrelevant: 777\n",
      "Recall Ratio: 0.5921052631578947  Precision Ratio: 0.5602716468590832\n",
      "F1 Score: 0.5757487641756325\n",
      "--------------------------------------------------------\n",
      "Training epoch: 12\n",
      "Training loss epoch: 0.08065493901570638\n",
      "Training accuracy epoch: 0.9752402213882408\n",
      "Name Entity Count: 7022, Name Entity Recognization: 6078, Name Entity Irrelevant: 920\n",
      "Recall Ratio: 0.8655653659925947  Precision Ratio: 0.8685338668190912\n",
      "F1 Score: 0.8670470756062767\n",
      "B-Auxiliary********************\n",
      "Name Entity Count: 139, Name Entity Recognization: 67, Name Entity Irrelevant: 77\n",
      "Recall Ratio: 0.48201438848920863  Precision Ratio: 0.4652777777777778\n",
      "F1 Score: 0.47349823321554774\n",
      "B-application********************\n",
      "Name Entity Count: 23, Name Entity Recognization: 2, Name Entity Irrelevant: 8\n",
      "Recall Ratio: 0.08695652173913043  Precision Ratio: 0.2\n",
      "F1 Score: 0.12121212121212122\n",
      "B-component********************\n",
      "Name Entity Count: 160, Name Entity Recognization: 101, Name Entity Irrelevant: 78\n",
      "Recall Ratio: 0.63125  Precision Ratio: 0.5642458100558659\n",
      "F1 Score: 0.5958702064896756\n",
      "B-material********************\n",
      "Name Entity Count: 206, Name Entity Recognization: 138, Name Entity Irrelevant: 83\n",
      "Recall Ratio: 0.6699029126213593  Precision Ratio: 0.6244343891402715\n",
      "F1 Score: 0.6463700234192037\n",
      "B-organization********************\n",
      "Name Entity Count: 49, Name Entity Recognization: 28, Name Entity Irrelevant: 17\n",
      "Recall Ratio: 0.5714285714285714  Precision Ratio: 0.6222222222222222\n",
      "F1 Score: 0.5957446808510639\n",
      "B-technology********************\n",
      "Name Entity Count: 211, Name Entity Recognization: 130, Name Entity Irrelevant: 112\n",
      "Recall Ratio: 0.6161137440758294  Precision Ratio: 0.5371900826446281\n",
      "F1 Score: 0.5739514348785871\n",
      "B-theory********************\n",
      "Name Entity Count: 884, Name Entity Recognization: 531, Name Entity Irrelevant: 379\n",
      "Recall Ratio: 0.6006787330316742  Precision Ratio: 0.5835164835164836\n",
      "F1 Score: 0.5919732441471572\n",
      "Validation Loss: 0.4905924261263664\n",
      "Validation Accuracy: 0.8856252600331803\n",
      "Name Entity Count: 1672, Name Entity Recognization: 997, Name Entity Irrelevant: 754\n",
      "Recall Ratio: 0.5962918660287081  Precision Ratio: 0.5693889206167904\n",
      "F1 Score: 0.5825299444931346\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)\n",
    "    labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc69c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_entity(labels):\n",
    "    name_entity_dict = {}\n",
    "    for key in ids_to_labels.keys():\n",
    "        if key%2==1:\n",
    "            name_entity_dict[ids_to_labels[key]]=set()\n",
    "    tem_label = None\n",
    "    for num, label in enumerate(labels):\n",
    "        if tem_label is not None:\n",
    "            if num == len(labels)-1 and label == tem_label + 1:\n",
    "                name_entity_dict[ids_to_labels[tem_label]].add((start, num+1))\n",
    "                break\n",
    "            if label != tem_label + 1:\n",
    "                name_entity_dict[ids_to_labels[tem_label]].add((start, num))\n",
    "                tem_label = None \n",
    "        if label % 2 != 0:\n",
    "            tem_label = label\n",
    "            start = num\n",
    "            if num == len(labels)-1:\n",
    "                name_entity_dict[ids_to_labels[tem_label]].add((start, num+1))\n",
    "                break\n",
    "    return name_entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa36226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得未标注的文章\n",
    "artical_df = pd.read_excel(r'C:\\Users\\us_Ma\\Desktop\\Share\\total_without_tag.xls')\n",
    "artical_df = artical_df.dropna(subset=['abstract'])\n",
    "for index, row in artical_df.iterrows():\n",
    "    artical_id = row[0]\n",
    "    artical_content = row[1]\n",
    "    if \"No abstract available\" in artical_content or \"For abstract, see\" in artical_content:\n",
    "        artical_df.drop(index, inplace=True)\n",
    "artical_df = artical_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2263047a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_entity_df = pd.DataFrame(columns=['name_entity','type','artical_id'])\n",
    "for index in range(len(total_set)):\n",
    "    labels = total_set[index]['labels']\n",
    "    ids = total_set[index]['input_ids']\n",
    "    mappings = total_set[index]['offset_mapping']\n",
    "    sentence = data.sentence[index]\n",
    "    artical_id = data.artical_id[index]    \n",
    "    active_label = []\n",
    "    for label, mapping in zip(labels.cpu().numpy(), mappings.squeeze().tolist()):\n",
    "        if mapping[0] == 0 and mapping[1] != 0:\n",
    "            active_label.append(label)\n",
    "        else:\n",
    "            continue\n",
    "    name_entity_dict = get_name_entity(active_label)\n",
    "    for key in name_entity_dict.keys():\n",
    "        for name_entity in name_entity_dict[key]:\n",
    "            name_entity_single = pd.DataFrame({\n",
    "                'name_entity' : ' '.join(sentence.split()[name_entity[0]:name_entity[1]]),\n",
    "                'type' : key,\n",
    "                'artical_id' : artical_id\n",
    "            }, index=[0])\n",
    "            name_entity_df = name_entity_df.append(name_entity_single, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "284430e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemical_convert(matched):\n",
    "    string = matched.group()\n",
    "    string = string.replace(' sub ', '_')\n",
    "    string = string.replace(' sup ', '^')\n",
    "    string = string.replace(' ','.')\n",
    "    return string\n",
    "\n",
    "def clear_sub(matched):\n",
    "    return ' sub '\n",
    "def clear_blacket(matched):\n",
    "    string = matched.group()\n",
    "    string = string.replace('(',' ')\n",
    "    string = string.replace(')',' ')\n",
    "    string = string.replace('  ',' ')\n",
    "    return string\n",
    "def txt_convent(txt_total):\n",
    "    txt_total = re.sub(r'\\\\sub ', clear_sub, txt_total)\n",
    "    txt_total = re.sub(r'\\/sub ', clear_sub, txt_total)\n",
    "    txt_total = re.sub('\\S+\\(su[bp] \\S+\\)', clear_blacket, txt_total)\n",
    "    txt_total = re.sub('\\S+ \\(su[bp] \\S+\\)', clear_blacket, txt_total)\n",
    "    txt_total = re.sub(r'\\S+ sub \\S+ \\S+ sub \\S+ \\S+ sub \\S+ \\S+ sub \\S+ \\S+ sub \\S+', chemical_convert, txt_total)\n",
    "    txt_total = re.sub(r'\\S+ sub \\S+ \\S+ sub \\S+ \\S+ sub \\S+ \\S+ sub \\S+', chemical_convert, txt_total)\n",
    "    txt_total = re.sub(r'\\S+ sub \\S+ \\S+ sub \\S+ \\S+ sub \\S+', chemical_convert, txt_total)\n",
    "    txt_total = re.sub(r'\\S+ sub \\S+ \\S+ sub \\S+', chemical_convert, txt_total)\n",
    "    txt_total = re.sub(r'\\S+ sup \\S+', chemical_convert, txt_total)\n",
    "    txt_total = re.sub(r'\\S+ sub \\S+', chemical_convert, txt_total)\n",
    "    txt_total = re.sub(r'\\S+ sub \\S+', chemical_convert, txt_total)\n",
    "    return txt_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a00b4159",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index, row in artical_df.iterrows():\n",
    "    artical_id = row[0]\n",
    "    artical_content = txt_convent(row[1])\n",
    "    if artical_id not in data.artical_id:\n",
    "        for sentence in nltk.sent_tokenize(artical_content):\n",
    "            sentence_tokenize = nltk.word_tokenize(sentence)\n",
    "            inputs = tokenizer(sentence_tokenize,\n",
    "                        is_split_into_words=True,\n",
    "                        return_offsets_mapping=True, \n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        max_length=MAX_LEN,\n",
    "                        return_tensors=\"pt\")\n",
    "            # move to gpu\n",
    "            ids = inputs[\"input_ids\"].to(device)\n",
    "            mask = inputs[\"attention_mask\"].to(device)\n",
    "            # forward pass\n",
    "            outputs = model(ids, attention_mask=mask)\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # model.num_labels: the numbers of labels \n",
    "            active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "\n",
    "            # torch.argmax returns the indices of the maximum value of all elements\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "            prediction = []\n",
    "            for pred, mapping in zip(flattened_predictions.cpu().numpy(), inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "                if mapping[0] == 0 and mapping[1] != 0:\n",
    "                    prediction.append(pred)\n",
    "                else:\n",
    "                    continue\n",
    "            name_entity_dict = get_name_entity(prediction)\n",
    "            for key in name_entity_dict.keys():\n",
    "                for name_entity in name_entity_dict[key]:\n",
    "                    name_entity_single = pd.DataFrame({\n",
    "                        'name_entity' : ' '.join(sentence_tokenize[name_entity[0]:name_entity[1]]),\n",
    "                        'type' : key,\n",
    "                        'artical_id' : artical_id\n",
    "                    }, index=[0])\n",
    "                    name_entity_df = name_entity_df.append(name_entity_single, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "356220da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaAs                         1071\n",
       "silicon                       733\n",
       "Si                            431\n",
       "defects                       275\n",
       "InP                           241\n",
       "                             ... \n",
       "floating gate devices           1\n",
       "MIOS                            1\n",
       "retention time                  1\n",
       "nonvolatile memory device       1\n",
       "ATOMINDEX                       1\n",
       "Name: name_entity, Length: 47812, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 20)\n",
    "name_entity_df.name_entity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b22e27a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_df = pd.read_excel(r'C:\\Users\\us_Ma\\Desktop\\Share\\total.xls')\n",
    "total_df = total_df[['paperid','publishdate']]\n",
    "total_df = total_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86b40bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_entity</th>\n",
       "      <th>type</th>\n",
       "      <th>artical_id</th>\n",
       "      <th>publishdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>electron gas</td>\n",
       "      <td>material</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>1985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dielectric function</td>\n",
       "      <td>technology</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>1985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EMC</td>\n",
       "      <td>technology</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>1985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ensemble Monte Carlo</td>\n",
       "      <td>technology</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>1985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scattering terms</td>\n",
       "      <td>technology</td>\n",
       "      <td>ADA154997</td>\n",
       "      <td>1985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99259</th>\n",
       "      <td>isotopes</td>\n",
       "      <td>theory</td>\n",
       "      <td>ZFK399</td>\n",
       "      <td>1980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99260</th>\n",
       "      <td>radioactive decay</td>\n",
       "      <td>theory</td>\n",
       "      <td>ZFK399</td>\n",
       "      <td>1980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99261</th>\n",
       "      <td>semiconductor detectors</td>\n",
       "      <td>component</td>\n",
       "      <td>ZFK399</td>\n",
       "      <td>1980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99262</th>\n",
       "      <td>detectors</td>\n",
       "      <td>component</td>\n",
       "      <td>ZFK399</td>\n",
       "      <td>1980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99263</th>\n",
       "      <td>precision spectrometry</td>\n",
       "      <td>technology</td>\n",
       "      <td>ZFK399</td>\n",
       "      <td>1980.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99264 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name_entity        type artical_id  publishdate\n",
       "0                 electron gas    material  ADA154997       1985.0\n",
       "1          dielectric function  technology  ADA154997       1985.0\n",
       "2                          EMC  technology  ADA154997       1985.0\n",
       "3         Ensemble Monte Carlo  technology  ADA154997       1985.0\n",
       "4             scattering terms  technology  ADA154997       1985.0\n",
       "...                        ...         ...        ...          ...\n",
       "99259                 isotopes      theory     ZFK399       1980.0\n",
       "99260        radioactive decay      theory     ZFK399       1980.0\n",
       "99261  semiconductor detectors   component     ZFK399       1980.0\n",
       "99262                detectors   component     ZFK399       1980.0\n",
       "99263   precision spectrometry  technology     ZFK399       1980.0\n",
       "\n",
       "[99264 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_entity_total_df = pd.merge(name_entity_df, total_df, left_on='artical_id', right_on='paperid',how='left')[['name_entity','type','artical_id','publishdate']]\n",
    "name_entity_total_df.type=name_entity_total_df.type.map(lambda x:x[2:])\n",
    "name_entity_total_df.to_csv(r'C:\\Users\\us_Ma\\Desktop\\Share\\name_entity.csv')\n",
    "name_entity_total_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
